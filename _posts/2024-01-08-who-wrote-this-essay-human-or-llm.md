---
layout: post
title: "Who Wrote This Essay, Human or LLM?"
date: 2024-01-08 09:00:00
---

For protecting plagiarism by LLM...

# 1.Project Definition
From [the competion overview](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/overview),

- **Project Overview** : This project, titled "LLM - Detect AI Generated Text," involves the development of a machine learning model capable of distinguishing between essays written by students and those generated by large language models (LLMs). The motivation behind the project is the increasing sophistication of LLMs, which raises concerns about their potential impact on education, particularly in terms of plagiarism and the alteration of students' skill development. The project is hosted on Kaggle in collaboration with Vanderbilt University and The Learning Agency Lab.
- **Problem Statement** : The main problem addressed in this project is the detection of AI-generated text, specifically essays produced by LLMs, and distinguishing them from essays authored by students. The proliferation of LLMs has led to concerns about potential academic issues such as plagiarism and the impact on students' learning. The task is to build a model that can accurately identify whether a given essay was written by a student or generated by an LLM.
- **Metrics** : The evaluation of the model's performance will be based on the area under the [**ROC curve**](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) . This metric provides a comprehensive measure of the model's ability to distinguish between student-written and LLM-generated essays. The choice of this metric aligns with the binary classification nature of the problem, and the ROC curve is particularly suitable for assessing the trade-off between true positive rate and false positive rate. Additionally, for the Efficiency Prize, an efficiency score is defined as a combination of model performance, runtime, and the absence of GPU usage. This score is designed to reward models that not only perform well but also demonstrate efficiency, addressing potential computational limitations in real-world educational contexts.

# 2.Analysis

From [the dataset description](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/data)

- **Data Exploration** : The competition dataset comprises about 10,000 essays, some written by students and some generated by a variety of large language models (LLMs). The goal of the competition is to determine whether or not essay was generated by an LLM. All of the essays were written in response to one of seven essay prompts. In each prompt, the students were instructed to read one or more source texts and then write a response. This same information may or may not have been provided as input to an LLM when generating an essay. Essays from two of the prompts compose the training set; the remaining essays compose the hidden test set. Nearly all of the training set essays were written by students, with only a few generated essays given as examples. You may wish to generate more essays to use as training data. Please note that this is a [Code Competition](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/overview/code-requirements). The data in `test_essays.csv` is only dummy data to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. There are about 9,000 essays in the test set, both student written and LLM generated.
    - **train_essays.csv / test_essays.csv**
        - `id` - A unique identifier for each essay.
        - `prompt_id` - Identifies the prompt the essay was written in response to.
        - `text` - The essay text itself.
        - `generated` - Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in `test_essays.csv`.
    - **train_prompts.csv** - Essays were written in response to information in these fields.
        - `prompt_id` - A unique identifier for each prompt.
        - `prompt_name` - The title of the prompt.
        - `instructions` - The instructions given to students.
        - `source_text` - The text of the article(s) the essays were written in response to, in Markdown format. Significant paragraphs are enumerated by a numeral preceding the paragraph on the same line, as in `0 Paragraph one.\n\n1 Paragraph two.`. Essays sometimes refer to a paragraph by its numeral. Each article is preceded with its title in a heading, like `# Title`. When an author is indicated, their name will be given in the title after `by`. Not all articles have authors indicated. An article may have subheadings indicated like `## Subheading`.
    - **sample_submission.csv** - A submission file in the correct format. See the [Evaluation](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/overview/evaluation) page for details
- **Data Visualization** :
  **The dataset provided was found to be unbalanced, consisting mostly of human-written text.**
  
  ![](/img/2024-01-08-distribution-of-dataset.png)

# 3.Methodology
- **Data Preprocessing**
    - Add extra dataset : The addition of [extra dataset](https://www.kaggle.com/code/suyashkapil/detecting-llm-generated-texts) eliminated the imbalance between classes.
    
      ![](/img/2024-01-08-distribution-of-extra-dataset.png)    
    - Drop duplicated : True
    - Drop na : False, there were no NA so no needed.
- **Implementation** :
    - Metrics : The Area Under the Receiver Operating Characteristic Curve (AUC-ROC) is a widely used metric for evaluating the performance of binary classification models. This metric assesses the model's ability to discriminate between positive and negative instances by plotting the true positive rate against the false positive rate across different probability thresholds.　In essence, the ROC curve is a graphical representation of a model's trade-off between sensitivity and specificity. The AUC-ROC score summarizes the area under this curve, providing a single scalar value that ranges from 0 to 1. A model with an AUC-ROC score of 0.5 indicates random chance, while a score of 1.0 signifies perfect discriminatory ability.　A higher AUC-ROC score indicates better model performance, illustrating the model's capacity to correctly rank positive instances higher than negative ones. This makes AUC-ROC particularly suitable for tasks where correctly identifying positive cases is crucial, such as medical diagnoses or fraud detection.
    - Algorithm and Technique : The decision to employ Random Forest as our primary classification algorithm and LightGBM as an alternative is rooted in their distinctive characteristics. Random Forest, being an ensemble method, excels in capturing complex relationships in the data. Its ability to handle non-linear patterns and mitigate overfitting aligns well with the intricate nature of our task. In contrast, LightGBM, a gradient boosting framework, is chosen for its efficiency in handling large datasets and its capacity to boost the performance of weak learners. For feature extraction, we utilize both CountVectorizer and tf-idfTransformer. CountVectorizer converts text into a bag-of-words representation, capturing the frequency of each term. On the other hand, tf-idfTransformer weighs the importance of terms based on their frequency across documents. This combination allows us to leverage both raw counts and term importance, providing a more comprehensive representation of textual data.
    - Parameters : In Random Forest, crucial parameters such as the number of trees in the ensemble and the maximum depth of each tree are carefully selected. With 100 trees, we strike a balance between model expressiveness and computational efficiency. The choice of n_estimators and max_depth is guided by the need to create a robust model capable of handling the complexities inherent in our dataset. In CountVectorizer, the ngram_range parameter is set to ((1, 1), (1, 2)), allowing the model to consider both unigrams and bigrams. This choice is motivated by the desire to capture not only individual words but also meaningful two-word combinations, providing a richer representation of the textual features.
    - Application to the Dataset : Random Forest and LightGBM are chosen for their adaptability to the characteristics of our dataset. The ensemble nature of Random Forest proves advantageous in handling imbalanced class distributions, ensuring reliable predictions across all classes. LightGBM's efficient training on large datasets is particularly valuable in scenarios where scalability is a concern, enhancing our ability to process extensive textual data efficiently. The inclusion of StartingVerbExtractor in our feature extraction pipeline addresses the specific linguistic characteristics of our text data. By identifying sentences that start with a verb, this custom feature extractor aims to capture syntactic patterns that might be indicative of certain classes in our classification task.
    - Complications of the Coding : In Grid Search, it was very time consuming to optimize many parameters such as n_estimators, max_depth, ngram_range with many choices, for example, `n_estimators=[1,10,10,100,1000,10000, ...]`, for example, would be very time-consuming to try to optimize. It was necessary to focus on important parameters.
- **Refinement** : 
    - Feature : Features of taring verb and text stats did not work well.
    - Classification : LightGBM classifier got high AUC score than Random forest classifier.
    - Grid search : `ngram_range` = (1, 1) was better.

# 4.Results
- **Model Evaluation and Validation** : Shown below. 
    - ROC curve(CountVectorizer + Tf-idf + RandomForest) with AUC: 0.9969649503271462
      
      ![](/img/2024-01-08-roc-base.png)  
    - Other pipelines were also trained and AUCs were calculated on test data. The features of staring-verb and text-stats did not work well.
      
      ![](/img/2024-01-08-auc.png)
- **Justification** : Looking at results of [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding), there is some degree of separation between the human-written essay and the LLM-written essay, although there are some areas that are not completely separated. I suspect that the pre-processing converted them into higher dimensional vectors, and that this worked well

  ![](/img/2024-01-08-t-sne-visualization.png)

# 5.Conclusion
- **Reflection** : By Tokenizing and embedding text, our model could classify essays written by human and essays written by LLM with high accuracy(AUC>=0.99). It was interesting to note that textual statistics such as the first verb in an essay, the number of words, sentences, and lines were not useful in classifying essays written by LLMs from those written by humans. The number of sentences appeared to be useful in classification, especially in the original dataset, the dataset with very little LLM data. However, when the data was increased and looked at in a balanced data set, it was not useful. We learned the importance of first considering increasing the amount of data in the case of small data sets. And our model was 3012nd in the competition.
  
  ![](/img/2024-01-08-submission.png)
  
- **Improvement** : Typo counts may work as a feature. This is because LLMs are thought not to make mistakes, whereas humans have many typos. Sentence structure and the similarity between the input prompt and the essay could also be features to distinguish.
  
# 6.Deliverables
- **Write-up or Application** : this blog post and [web app](https://github.com/soyaoki/DSND-Capstone/tree/main).
- **Github Repository** : repo of [web app](https://github.com/soyaoki/DSND-Capstone/tree/main).
- **Best Practices** : [Code](https://github.com/soyaoki/DSND-Capstone/blob/main/models/train_classifier.py) with DRY principles and PEP8.
