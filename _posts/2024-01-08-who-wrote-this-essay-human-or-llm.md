---
layout: post
title: "Who Wrote This Essay, Human or LLM?"
date: 2024-01-08 09:00:00
---

# 1.Project Definition
From [the competion overvies](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/overview),
- **Project Overview** : This project, titled "LLM - Detect AI Generated Text," involves the development of a machine learning model capable of distinguishing between essays written by students and those generated by large language models (LLMs). The motivation behind the project is the increasing sophistication of LLMs, which raises concerns about their potential impact on education, particularly in terms of plagiarism and the alteration of students' skill development. The project is hosted on Kaggle in collaboration with Vanderbilt University and The Learning Agency Lab.
    
- **Problem Statement** : The main problem addressed in this project is the detection of AI-generated text, specifically essays produced by LLMs, and distinguishing them from essays authored by students. The proliferation of LLMs has led to concerns about potential academic issues such as plagiarism and the impact on students' learning. The task is to build a model that can accurately identify whether a given essay was written by a student or generated by an LLM.

    
- **Metrics** : The evaluation of the model's performance will be based on **[the area under the ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve**. This metric provides a comprehensive measure of the model's ability to distinguish between student-written and LLM-generated essays. The choice of this metric aligns with the binary classification nature of the problem, and the ROC curve is particularly suitable for assessing the trade-off between true positive rate and false positive rate.

    Additionally, for the Efficiency Prize, an efficiency score is defined as a combination of model performance, runtime, and the absence of GPU usage. This score is designed to reward models that not only perform well but also demonstrate efficiency, addressing potential computational limitations in real-world educational contexts.

# 2.Analysis
From [the dataset description](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/data)
- **Data Exploration** : The competition dataset comprises about 10,000 essays, some written by students and some generated by a variety of large language models (LLMs). The goal of the competition is to determine whether or not essay was generated by an LLM.
    All of the essays were written in response to one of seven essay prompts. In each prompt, the students were instructed to read one or more source texts and then write a response. This same information may or may not have been provided as input to an LLM when generating an essay.
    Essays from two of the prompts compose the training set; the remaining essays compose the hidden test set. Nearly all of the training set essays were written by students, with only a few generated essays given as examples. You may wish to generate more essays to use as training data.
    Please note that this is a [Code Competition](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/overview/code-requirements). The data in `test_essays.csv` is only dummy data to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. There are about 9,000 essays in the test set, both student written and LLM generated.
    
    - **{test|train}_essays.csv**
        - `id` - A unique identifier for each essay.
        - `prompt_id` - Identifies the prompt the essay was written in response to.
        - `text` - The essay text itself.
        - `generated` - Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in `test_essays.csv`.
    - **train_prompts.csv** - Essays were written in response to information in these fields.
        - `prompt_id` - A unique identifier for each prompt.
        - `prompt_name` - The title of the prompt.
        - `instructions` - The instructions given to students.
        - `source_text` - The text of the article(s) the essays were written in response to, in Markdown format. Significant paragraphs are enumerated by a numeral preceding the paragraph on the same line, as in `0 Paragraph one.\n\n1 Paragraph two.`. Essays sometimes refer to a paragraph by its numeral. Each article is preceded with its title in a heading, like `# Title`. When an author is indicated, their name will be given in the title after `by`. Not all articles have authors indicated. An article may have subheadings indicated like `## Subheading`.
    - **sample_submission.csv** - A submission file in the correct format. See the [Evaluation](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/overview/evaluation) page for details
    
    
- **Data Visualization** :
  **The dataset provided was found to be unbalanced, consisting mostly of human-written text.**
  # [image1: Distribution of dataset]

# 3.Methodology
- **Data Preprocessing** 
    - Add extra dataset : [Other dataset](https://www.kaggle.com/code/suyashkapil/detecting-llm-generated-texts)
    - Drop duplicated : True
    - Drop na : False, there were no NA so no needed.
- **Implementation** : 
    - Metric : AUC by [sklearn.metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)
    - Pipeline : 
        - Feature extraction
            - Tokenizing text : Clean text, including lowercasing and removal of stop words in fuction `tokenize()`
            - Embedding text : `CountVector()` and `tf-idfTransfomer()` by scikit-learn
            - Detecting staring verb : `StartingVerbExtractor()`
            - Counting number of words, sentences and lines : Count the stats as log scale in `TextStatsExtractor()` 
        - Classification
            - `Random forest` or `LightGBM`
    - Other technics :     
        - Grid search : `ngram_range` in CountVectorizer, ((1, 1), (1, 2))
- **Refinement** : 
    - Feature : staring verb and text stats did not work well.
    - Classification : LightGBM > Random forest
    - Grid search : `ngram_range` = (1, 1)

# 4.Results
- **Model Evaluation and Validation** : Shown below
# [image2: distribution of extra dataset]
# [image3: AUC]
# [image4: experiment]

- **Justification** : Visualization by t-SNE
# [image5: t-sne]

# 5.Conclusion
- Reflection : 
- Improvement :

# 6.Deliverables
- Write-up or Application : this and repo
- Github Repository : repo
- Best Practices : DRY principles and PEP8
